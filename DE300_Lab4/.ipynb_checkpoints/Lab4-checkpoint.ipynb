{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff781e6-0c0f-45da-a6ac-cbdc74b3765b",
   "metadata": {},
   "source": [
    "## High-Level ETL (Extract - Transform - Load) Flow\n",
    "**Goal**: By the end of this tutorial, you will be able to\n",
    "- Extract: Download a file from AWS S3 using Pythonâ€™s boto3.\n",
    "- Transform: Clean, filter, or manipulate data in Python (often using libraries like pandas).\n",
    "- Load: Insert the transformed data into a relational database via SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4c364-5538-4205-bc71-4a9e631eeefc",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "\n",
    "1. Implement the following functions\n",
    "   - `extract_from_csv(file_to_process: str) -> pd.DataFrame`: read the .csv file and return dataframe\n",
    "   - `extract_from_json(file_to_process: str) -> pd.DataFrame`: read the .json file and return dataframe\n",
    "   - `extract() -> pd.DataFrame`: extract data of heterogeneous format and combine them into a single dataframe.\n",
    "   - `transform(df) -> pd.DataFrame`: function for data cleaning and manipulation.\n",
    "2. Clean the data\n",
    "   - Round float-type columns to two decimal places.\n",
    "   - remove duplicate samples\n",
    "   - Save the cleaned data into parquet file\n",
    "3. Insert the data into SQL\n",
    "   - Create postgresql database\n",
    "   - Insert the data into the database\n",
    "  \n",
    "Submission requirement:\n",
    "    1. Jupyter Notebook\n",
    "    2. Parquet File\n",
    "    3. SQL file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9594048-5e1f-4138-b202-30eabce92d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Package:\n",
    "# psycopg2 2.9.10 (A PostgreSQL database adapter)\n",
    "# pandas 2.0.3 (For data manipulation and analysis)\n",
    "# sqlalchemy 2.0.37 (A SQL toolkit and Object Relational Mapper)\n",
    "# pyarrow 14.0.1 (Provides support for efficient in-memory columnar data structures, part from Apache Arrow Objective)\n",
    "import pandas as pd\n",
    "\n",
    "#required for reading .xml files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#required for navigating machine's directory\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "#required for communicating with SQL database\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6889e8d0-b1ac-4387-9ef6-f403cb2343dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.36.16-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.37.0,>=1.36.16 (from boto3)\n",
      "  Downloading botocore-1.36.16-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from botocore<1.37.0,>=1.36.16->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from botocore<1.37.0,>=1.36.16->boto3) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.16->boto3) (1.16.0)\n",
      "Downloading boto3-1.36.16-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.2 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/139.2 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/139.2 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 41.0/139.2 kB 330.3 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 41.0/139.2 kB 330.3 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 41.0/139.2 kB 330.3 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 102.4/139.2 kB 454.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 139.2/139.2 kB 458.9 kB/s eta 0:00:00\n",
      "Downloading botocore-1.36.16-py3-none-any.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/13.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ---------------------------------------- 0.2/13.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ---------------------------------------- 0.2/13.3 MB 2.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/13.3 MB 2.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.1/13.3 MB 4.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.0/13.3 MB 6.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/13.3 MB 7.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/13.3 MB 7.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.8/13.3 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.4/13.3 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.0/13.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.2/13.3 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.5/13.3 MB 7.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.1/13.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.0/13.3 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.6/13.3 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.9/13.3 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.9/13.3 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.3/13.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.4/13.3 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.6/13.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.0/13.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.1/13.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.2/13.3 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.5/13.3 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.2/13.3 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.3/13.3 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.9/13.3 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.7/13.3 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.7/13.3 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.7/13.3 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.8/13.3 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.9/13.3 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.9/13.3 MB 7.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.3/13.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.8/13.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.2/13.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.2/84.2 kB ? eta 0:00:00\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.69\n",
      "    Uninstalling botocore-1.34.69:\n",
      "      Successfully uninstalled botocore-1.34.69\n",
      "Successfully installed boto3-1.36.16 botocore-1.36.16 s3transfer-0.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.36.16 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ea5ad-4aa4-4d75-8cca-41de1e7454f3",
   "metadata": {},
   "source": [
    "# E: Extracting data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5964aa80-e1d5-42bf-8159-6ad91273a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "my_aws_access_key_id='ASIAYAAO5HRMLXI6IMLK'\n",
    "my_aws_secret_access_key='oEO9uc7z0Iggmheg9GiFLZ2v6yFIH/Xvjy8qlKgG'\n",
    "my_aws_session_token='IQoJb3JpZ2luX2VjEHsaCXVzLWVhc3QtMiJGMEQCIA9FSi1eY2bChnH2oSc9c/HlPjysTNCjczuw+Y9D1qAMAiAOs5afOMtlX+VHVhhOYl61bSV0fXh/Fl1GY6M/+lLERir0AgiV//////////8BEAAaDDU0OTc4NzA5MDAwOCIM8iWjHs/tE68hG0bdKsgC+POvBxqBSY2PcQovXNWJqZy+ps0i+JToPn4N9aoDAb0oqkqtM5uBFYho3RJb9420irSZq3RRcx71N5rjDK7/ig/a6QeFqMkeXPOtWuDSsJMsRuXQ9zGESZz9F1exNscJzgbXxIcO9VaOoOfRqCiPhoeQJMkhHnKh6eKq3srEKCDZ4J0XUt8dG4PRohsEFMsp/ew0eWXFFKJ7QGaFT516gPiupVslcJYtiQZCet7V0bSHv+eN1rU3NPMFVzfpKV/nL4SJlA1JicrEUTLct7TVE10FOx6wRbpi10PW6pLACkJv6qrAMU/Vnzn0PNkdan3t11IMlttMJKT9Dmxsz6tf7KPehL8kL86rgd3wBhukcR9Pe8wN9ydObQgpfH4m5mZwFETpanty0xCOqo9hoJIYhiCul540rALvgTNG4QbqSoeEwpvulOsnTzCN4J69BjqoAXw+VzazjVPdW8/bg21myKsufABK9gRh7Kz5VFhadDgOl7VgrT9k3tZew0i2+HCO/RrAvZVI9G0ToXndRZ7Ow+F3XY7Q3hfTDbmMEb1JQ4M2Aru2zv70N2iKnXZHOG6SoAHeFkCPqEesth0ueJ+GGb4vE6iNTRQl/BbcLKfQm1+V4lDhSti6qdbMoRWCMQJ87SkuIEdQS1lS6ObTOoCby/8dLcuvqskvCw=='\n",
    "\n",
    "BUCKET_NAME = 'de300winter2025'   # Replace with your bucket name\n",
    "S3_FOLDER = 'dinglin_xia/lab4_data/'             # The folder path in S3\n",
    "LOCAL_DIR = 'C:/Users/nebah/Downloads/DE300_Lab4'      # Local directory to save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3161fa0-3999-4aff-a1e8-e0f364fd5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_folder(bucket_name, s3_folder, local_dir):\n",
    "    \"\"\"Download a folder from S3.\"\"\"\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    # List objects within the specified folder\n",
    "    s3_resource = boto3.resource('s3',\n",
    "                                aws_access_key_id=my_aws_access_key_id,\n",
    "                                aws_secret_access_key=my_aws_secret_access_key,\n",
    "                                aws_session_token=my_aws_session_token)\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        # Define local file path\n",
    "        local_file_path = os.path.join(local_dir, obj.key[len(s3_folder):])  \n",
    "        \n",
    "        if obj.key.endswith('/'):  # Skip folders\n",
    "            continue\n",
    "        \n",
    "        # Create local directory if needed\n",
    "        local_file_dir = os.path.dirname(local_file_path)\n",
    "        if not os.path.exists(local_file_dir):\n",
    "            os.makedirs(local_file_dir)\n",
    "        \n",
    "        # Download the file\n",
    "        bucket.download_file(obj.key, local_file_path)\n",
    "        print(f\"Downloaded {obj.key} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca57a83a-ceb6-4cb8-9101-75426a5728f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.csv to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices1.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.json to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices1.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.xml to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices1.xml\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.csv to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices2.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.json to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices2.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.xml to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices2.xml\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.csv to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices3.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.json to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices3.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.xml to C:/Users/nebah/Downloads/DE300_Lab4\\used_car_prices3.xml\n"
     ]
    }
   ],
   "source": [
    "download_s3_folder(BUCKET_NAME, S3_FOLDER, LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14027a8-8a6d-4566-93ba-710e68b97f3e",
   "metadata": {},
   "source": [
    "## Extract data from ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03b2bdcb-fb78-419c-a3e9-9f12ee8f2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\used_car_prices1.csv\n",
      "./data\\used_car_prices1.json\n",
      "./data\\used_car_prices1.xml\n",
      "./data\\used_car_prices2.csv\n",
      "./data\\used_car_prices2.json\n",
      "./data\\used_car_prices2.xml\n",
      "./data\\used_car_prices3.csv\n",
      "./data\\used_car_prices3.json\n",
      "./data\\used_car_prices3.xml\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('./data/*')\n",
    "\n",
    "# Output the list of files\n",
    "for file in all_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30b5a8-7206-4a8c-8837-b42ab76115fb",
   "metadata": {},
   "source": [
    "### Function to extract data from one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ac06a7b-ea5d-4410-b0b3-be9db100d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process: str) -> pd.DataFrame:\n",
    "    \n",
    "    # add you line here to read the .csv file and return dataframe\n",
    "    return pd.read_csv(file_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051e6b2-6f3a-4b2c-9ff4-1e10a91a296e",
   "metadata": {},
   "source": [
    "### Function to extract data from one .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31b19053-3d8a-4386-82d9-5a7b287d37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(file_to_process: str) -> pd.DataFrame:\n",
    "    \n",
    "    # add you line here to read the .json file and return dataframe\n",
    "    try:\n",
    "        # Attempt to read as JSON Lines first.\n",
    "        return pd.read_json(file_to_process, lines=True)\n",
    "    except ValueError as e:\n",
    "        # If that fails, try the standard read_json.\n",
    "        print(f\"Could not read {file_to_process} with lines=True: {e}\")\n",
    "        return pd.read_json(file_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072dfa48-6805-451e-9d0f-eb09de70e9b1",
   "metadata": {},
   "source": [
    "### Function to extract data from one  .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f6bd897-d626-43f3-9980-e3c1e0e27ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_xml(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.DataFrame(columns = columns)\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        car_model = person.find(\"car_model\").text\n",
    "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
    "        price = float(person.find(\"price\").text)\n",
    "        fuel = person.find(\"fuel\").text\n",
    "        sample = pd.DataFrame({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, index = [0])\n",
    "        dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2e693-0d74-4e9e-92e1-395119aab10e",
   "metadata": {},
   "source": [
    "### Function to extract data from the ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7fbc266d-1ef2-49bb-8784-4a8ccbffd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract() -> pd.DataFrame:\n",
    "    # Create a list to store DataFrames from different files.\n",
    "    data_frames = []\n",
    "    \n",
    "    # Process CSV files\n",
    "    for csv_file in glob.glob(os.path.join(folder, \"*.csv\")):\n",
    "        try:\n",
    "            df_csv = extract_from_csv(csv_file)\n",
    "            data_frames.append(df_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file {csv_file}: {e}\")\n",
    "    \n",
    "    # Process JSON files\n",
    "    for json_file in glob.glob(os.path.join(folder, \"*.json\")):\n",
    "        try:\n",
    "            df_json = extract_from_json(json_file)\n",
    "            data_frames.append(df_json)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading JSON file {json_file}: {e}\")\n",
    "    \n",
    "    # Process XML files (requires pandas 1.3.0+)\n",
    "    for xml_file in glob.glob(os.path.join(folder, \"*.xml\")):\n",
    "        try:\n",
    "            df_xml = pd.read_xml(xml_file)\n",
    "            data_frames.append(df_xml)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading XML file '{xml_file}': {e}\")\n",
    "    \n",
    "    # If we have extracted any data, concatenate the DataFrames.\n",
    "    if data_frames:\n",
    "        extracted_data = pd.concat(data_frames, ignore_index=True)\n",
    "        # Ensure the DataFrame contains only the desired columns (if possible)\n",
    "        # This will raise a KeyError if any expected column is missing.\n",
    "        try:\n",
    "            extracted_data = extracted_data[columns]\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Not all expected columns are present. {e}\")\n",
    "    else:\n",
    "        extracted_data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cafa18-245e-4e2c-a63b-6854eb93b863",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a70a8ce-348f-4b26-8a9a-3ead42565c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['car_model','year_of_manufacture','price', 'fuel']\n",
    "folder = \"./data\"\n",
    "#table_name = \"car_data\"\n",
    "\n",
    "# run\n",
    "def main():\n",
    "    data = extract()\n",
    "    #insert_to_table(data, \"car_data\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b1288a8-86fb-498c-833a-682ecd9c1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.552239</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.895522</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.731343</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.671642</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model  year_of_manufacture         price    fuel\n",
       "0      ritz                 2014   5000.000000  Petrol\n",
       "1       sx4                 2013   7089.552239  Diesel\n",
       "2      ciaz                 2017  10820.895522  Petrol\n",
       "3   wagon r                 2011   4253.731343  Petrol\n",
       "4     swift                 2014   6865.671642  Diesel"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265c408-8c63-413f-ad04-8d391e5b564d",
   "metadata": {},
   "source": [
    "# T: Transformation data and save organized data to .parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "faae9e13-ded5-47e0-8316-13ec6084ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_file = \"cars.parquet\"\n",
    "staging_data_dir = \"staging_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eefdc72e-6359-4363-a648-86082888f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # truncate price with 2 decimal place (add your code below)\n",
    "    if 'price' in df.columns:\n",
    "        df['price'] = df['price'].round(2)\n",
    "\n",
    "    # remove samples with same car_model (add your code below)\n",
    "    if 'car_model' in df.columns:\n",
    "        df = df.drop_duplicates(subset=['car_model'], keep='first')\n",
    "    \n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # Ensure the staging directory exists before writing the Parquet file\n",
    "    if not os.path.exists(staging_data_dir):\n",
    "        os.makedirs(staging_data_dir)\n",
    "        print(f\"Directory '{staging_data_dir}' created.\")\n",
    "\n",
    "    # write to parquet\n",
    "    df.to_parquet(os.path.join(staging_data_dir, staging_file))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f5d5423-c198-4aa7-89f7-354992f97324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (90, 4)\n",
      "Shape of data (25, 4)\n",
      "Directory 'staging_data' created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.55</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.90</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.73</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.67</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model  year_of_manufacture     price    fuel\n",
       "0      ritz                 2014   5000.00  Petrol\n",
       "1       sx4                 2013   7089.55  Diesel\n",
       "2      ciaz                 2017  10820.90  Petrol\n",
       "3   wagon r                 2011   4253.73  Petrol\n",
       "4     swift                 2014   6865.67  Diesel"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the head of your data\n",
    "df = transform(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960d531-a731-42d4-906c-4602391a16ca",
   "metadata": {},
   "source": [
    "# L: Loading data for further modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01f8d-a8b6-454c-a842-61c44cf04efc",
   "metadata": {},
   "source": [
    "### Set Up PostgreSQL Locally\n",
    "#### Step 1: Install PostgreSQL\n",
    "- Windows: Download from MySQL Official Site {https://www.postgresql.org/download/}\n",
    "- Mac:\n",
    "  ```{bash}\n",
    "  brew install postgresql\n",
    "  brew services start postgresql\n",
    "  ```\n",
    "Then access PostgreSQL CLI\n",
    "```{bash}\n",
    "psql -U postgres\n",
    "```\n",
    "Note: if you don't have default \"postgres\" user, then create it manually by\n",
    "```{bash}\n",
    "default \"postgres\" user\n",
    "```\n",
    "or\n",
    "```{bash}\n",
    "sudo -u $(whoami) createuser postgres -s\n",
    "```\n",
    "\n",
    "Then create a database\n",
    "```{sql}\n",
    "CREATE DATABASE my_local_db;\n",
    "\\l  -- List all databases\n",
    "```\n",
    "\n",
    "#### Step 2: Create a User and Grant Privileges\n",
    "In PostgreSQL CLI:\n",
    "```{sql}\n",
    "CREATE USER myuser WITH ENCRYPTED PASSWORD 'mypassword';\n",
    "GRANT ALL PRIVILEGES ON DATABASE my_local_db TO myuser;\n",
    "```\n",
    "\n",
    "#### Step 3: Install Required Python Libraries\n",
    "```{bash}\n",
    "pip install pandas sqlalchemy pymysql psycopg2 mysql-connector-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7ec1fba2-182f-4cc0-a028-d8008ab3d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nebah\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\nebah\\anaconda3\\lib\\site-packages (2.0.30)\n",
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nebah\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 41.0/45.0 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.0/45.0 kB 560.2 kB/s eta 0:00:00\n",
      "Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl (16.1 MB)\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.1 MB 9.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.1/16.1 MB 11.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/16.1 MB 12.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.7/16.1 MB 14.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 3.1/16.1 MB 13.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.6/16.1 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 4.0/16.1 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.9/16.1 MB 13.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.6/16.1 MB 13.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 6.0/16.1 MB 13.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.9/16.1 MB 13.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 7.1/16.1 MB 13.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 8.2/16.1 MB 13.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.6/16.1 MB 13.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.6/16.1 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.1/16.1 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.5/16.1 MB 13.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.9/16.1 MB 13.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.0/16.1 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.5/16.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 12.0/16.1 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.3/16.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.7/16.1 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.1/16.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/16.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.6/16.1 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.2/16.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.1/16.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.1/16.1 MB 11.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pymysql, psycopg2, mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.2.0 psycopg2-2.9.10 pymysql-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy pymysql psycopg2 mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270a29e-c73a-4ca1-b7e8-64ec621cb29a",
   "metadata": {},
   "source": [
    "### Utility function for writing data into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6f188de2-ae27-444c-8590-4c1dde9ac7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_host = \"localhost:5432\"\n",
    "db_user = \"postgres\"\n",
    "db_password = \"dezo015577\"\n",
    "db_name = \"my_local_db\"\n",
    "\n",
    "conn_string = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7cdb749a-4467-4264-b23b-f0693ee980af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            schemaname                tablename tableowner tablespace  \\\n",
      "0           pg_catalog             pg_statistic   postgres       None   \n",
      "1           pg_catalog                  pg_type   postgres       None   \n",
      "2           pg_catalog         pg_foreign_table   postgres       None   \n",
      "3           pg_catalog                pg_authid   postgres  pg_global   \n",
      "4           pg_catalog    pg_statistic_ext_data   postgres       None   \n",
      "..                 ...                      ...        ...        ...   \n",
      "63          pg_catalog           pg_largeobject   postgres       None   \n",
      "64  information_schema                sql_parts   postgres       None   \n",
      "65  information_schema             sql_features   postgres       None   \n",
      "66  information_schema  sql_implementation_info   postgres       None   \n",
      "67  information_schema               sql_sizing   postgres       None   \n",
      "\n",
      "    hasindexes  hasrules  hastriggers  rowsecurity  \n",
      "0         True     False        False        False  \n",
      "1         True     False        False        False  \n",
      "2         True     False        False        False  \n",
      "3         True     False        False        False  \n",
      "4         True     False        False        False  \n",
      "..         ...       ...          ...          ...  \n",
      "63        True     False        False        False  \n",
      "64       False     False        False        False  \n",
      "65       False     False        False        False  \n",
      "66       False     False        False        False  \n",
      "67       False     False        False        False  \n",
      "\n",
      "[68 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test connection\n",
    "df = pd.read_sql(\"SELECT * FROM pg_catalog.pg_tables;\", con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f2fed596-5b37-4da6-927d-facc292e733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_table(data: pd.DataFrame, conn_string:str, table_name:str):\n",
    "    db = create_engine(conn_string) # creates a connection to the database using SQLAlchemy\n",
    "    conn = db.connect() # Establishes a database connection\n",
    "    data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "db20ac0d-c828-45a1-b29a-a8cf20e5cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n"
     ]
    }
   ],
   "source": [
    "# read from the .parquet file\n",
    "\n",
    "def load() -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    for parquet_file in glob.glob(os.path.join(staging_data_dir, \"*.parquet\")):\n",
    "        data = pd.concat([pd.read_parquet(parquet_file),data])\n",
    "\n",
    "    #insert_to_table(data, table_name)\n",
    "    insert_to_table(data = data, conn_string = conn_string, table_name = 'ml_car_data')\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71df5f-6ba3-4b88-a9e7-a66e3d8aeeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
