{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea7ead5-3dc7-4838-8ab8-a82ac02753c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.36.17-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.37.0,>=1.36.17 (from boto3)\n",
      "  Downloading botocore-1.36.17-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Using cached s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.17->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.17->boto3) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.17->boto3) (1.16.0)\n",
      "Downloading boto3-1.36.17-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.36.17-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.36.17 botocore-1.36.17 jmespath-1.0.1 s3transfer-0.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9490ba1f-0a26-4cd6-9fdf-d9571717ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age  sex  painloc  painexer  relrest  pncaden   cp  trestbps  htn   chol  \\\n",
      "0  63  1.0      NaN       NaN      NaN      NaN  1.0     145.0  1.0  233.0   \n",
      "1  67  1.0      NaN       NaN      NaN      NaN  4.0     160.0  1.0  286.0   \n",
      "2  67  1.0      NaN       NaN      NaN      NaN  4.0     120.0  1.0  229.0   \n",
      "3  37  1.0      NaN       NaN      NaN      NaN  3.0     130.0  0.0  250.0   \n",
      "4  41  0.0      NaN       NaN      NaN      NaN  2.0     130.0  1.0  204.0   \n",
      "\n",
      "   ...  exeref  exerwm  thal  thalsev  thalpul  earlobe  cmo  cday   cyr  \\\n",
      "0  ...     NaN     NaN   6.0      NaN      NaN      NaN  2.0  16.0  81.0   \n",
      "1  ...     NaN     NaN   3.0      NaN      NaN      NaN  2.0   5.0  81.0   \n",
      "2  ...     NaN     NaN   7.0      NaN      NaN      NaN  2.0  20.0  81.0   \n",
      "3  ...     NaN     NaN   3.0      NaN      NaN      NaN  2.0   4.0  81.0   \n",
      "4  ...     NaN     NaN   3.0      NaN      NaN      NaN  2.0  18.0  81.0   \n",
      "\n",
      "   target  \n",
      "0     0.0  \n",
      "1     1.0  \n",
      "2     1.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading required package\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "# you need to change the credentials for yourself\n",
    "# Note that aws_access_key_id changes from time to time\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id='ASIAYAAO5HRMGMR6VLZG',\n",
    "                  aws_secret_access_key='6kXSuniH1tE8XQDZortpmWPQqaUMhKC0WDgc9as1',\n",
    "                  aws_session_token='IQoJb3JpZ2luX2VjELL//////////wEaCXVzLWVhc3QtMiJGMEQCIBDmD+Pm4X75Q62yHc22OhaOezPh9CWKzc4swGfjhFxwAiAK9YDgC44yLjFDVdC7gx4wc34EsDf2qWnIDbs9TQwRDyr0AgjL//////////8BEAAaDDU0OTc4NzA5MDAwOCIMJ5I5/CAlMXQx/UXIKsgCquc02CyEB8ABbd7EohtIeui4OXZycQDY+peBRj7ZQrYVJ8+5Z4u5AnUWJAH+iXXyZAhVq+pzixQJJJK4hfA98jk+MVM5mgN4NjvfTqwW5x5vXJH58MYyvE7l1ZtoNja47wAl7DxTnsKEkBcmw0diEOrTuz7MRbOXlzcCae9kqqJyhD2Ji9I17z/PK+UCJllXlWHU5CsivfMHwhPmvY+ZEeq0VwOmUadaQyfNigFOmSUja7SRZ1L9whuvaOdFOX/PA4TG4Si4P48AclpLx2YUThLTi0S0LWkUeoQrpCmPUT46mOb8iciarLkbeUrctjPc/viOI8uqPudnZ5aCWYYeIRDIhwgTLkQTYSh4DBr0Ki+ZWrCSPZOKtwBC/IeeNpblASyB56U+f6cGK2SJH/6pjInONQGOwqCEBZ7EXRbpcGFQfL5JAYIGOTCa5Kq9BjqoAWYZUU1O1EvpQnf5xl/tbcJ1sN9QSjHpv/XzFlxzXI9CyaMhTPcaWtzjPzb32LvQ/LOKR6YzXZRfx3Dqux8gcdlIhTk+r5aY0ldG55jMgz6SHD4GiuFaBXWZyYkSccAo5BQMxoOajV5uEgb90n04yiY+8EeDr0aaPnxSuIJyEmH30L8S94TUtDu18bDYLK3ltvFoRZR5Uo2VJVcKObUwoLPzK0r3WwUaoA==')\n",
    "\n",
    "\n",
    "bucket_name = 'de300winter2025'\n",
    "object_key = 'desmond_nebah/heart_disease.csv'\n",
    "csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "df = pd.read_csv(BytesIO(csv_string.encode()))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97699497-9f45-4ab5-a5f9-fde07d7d9241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age  sex  painloc  painexer   cp  trestbps  smoke  fbs  prop  nitr  pro  \\\n",
      "0  63  1.0      NaN       NaN  1.0     145.0    NaN  1.0   0.0   0.0  0.0   \n",
      "1  67  1.0      NaN       NaN  4.0     160.0    NaN  0.0   1.0   0.0  0.0   \n",
      "2  67  1.0      NaN       NaN  4.0     120.0    NaN  0.0   1.0   0.0  0.0   \n",
      "3  37  1.0      NaN       NaN  3.0     130.0    NaN  0.0   1.0   0.0  0.0   \n",
      "4  41  0.0      NaN       NaN  2.0     130.0    NaN  0.0   0.0   0.0  0.0   \n",
      "\n",
      "   diuretic  thaldur  thalach  exang  oldpeak  slope  \n",
      "0       0.0     10.5    150.0    0.0      2.3    3.0  \n",
      "1       0.0      9.5    108.0    1.0      1.5    2.0  \n",
      "2       0.0      8.5    129.0    1.0      2.6    2.0  \n",
      "3       0.0     13.0    187.0    0.0      3.5    3.0  \n",
      "4       0.0      7.0    172.0    0.0      1.4    1.0  \n"
     ]
    }
   ],
   "source": [
    "# Cleaning the Data\n",
    "\n",
    "# Cleaning Step 1\n",
    "\n",
    "# List of columns to retain\n",
    "columns_to_retain = [\n",
    "    'age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs',\n",
    "    'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope']\n",
    "\n",
    "# Retaining the specified columns\n",
    "df_cleaned = df[columns_to_retain]\n",
    "\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ecd722d-da68-4a23-80ff-a36df5a6c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age  sex  painloc  painexer   cp  trestbps  smoke  fbs  prop  nitr  pro  \\\n",
      "0  63  1.0      1.0       1.0  1.0     145.0    NaN  1.0   0.0   0.0  0.0   \n",
      "1  67  1.0      1.0       1.0  4.0     160.0    NaN  0.0   1.0   0.0  0.0   \n",
      "2  67  1.0      1.0       1.0  4.0     120.0    NaN  0.0   1.0   0.0  0.0   \n",
      "3  37  1.0      1.0       1.0  3.0     130.0    NaN  0.0   1.0   0.0  0.0   \n",
      "4  41  0.0      1.0       1.0  2.0     130.0    NaN  0.0   0.0   0.0  0.0   \n",
      "\n",
      "   diuretic  thaldur  thalach  exang  oldpeak  slope  \n",
      "0       0.0     10.5    150.0    0.0      2.3    3.0  \n",
      "1       0.0      9.5    108.0    1.0      1.5    2.0  \n",
      "2       0.0      8.5    129.0    1.0      2.6    2.0  \n",
      "3       0.0     13.0    187.0    0.0      3.5    3.0  \n",
      "4       0.0      7.0    172.0    0.0      1.4    1.0  \n"
     ]
    }
   ],
   "source": [
    "# Cleaning Step 2\n",
    "\n",
    "# Creating a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "df_cleaned = df[columns_to_retain].copy()\n",
    "\n",
    "# Step 1: Impute missing values for 'painloc' and 'painexer'\n",
    "df_cleaned.loc[:, 'painloc'] = df_cleaned['painloc'].fillna(df_cleaned['painloc'].mode()[0])\n",
    "df_cleaned.loc[:, 'painexer'] = df_cleaned['painexer'].fillna(df_cleaned['painexer'].mode()[0])\n",
    "\n",
    "# Step 2: Replace values less than 100 mm Hg for 'trestbps'\n",
    "df_cleaned.loc[:, 'trestbps'] = df_cleaned['trestbps'].apply(lambda x: x if x >= 100 else df_cleaned['trestbps'].median())\n",
    "\n",
    "# Step 3: Replace values less than 0 and greater than 4 for 'oldpeak'\n",
    "df_cleaned.loc[:, 'oldpeak'] = df_cleaned['oldpeak'].apply(lambda x: x if 0 <= x <= 4 else df_cleaned['oldpeak'].median())\n",
    "\n",
    "# Step 4: Impute missing values for 'thaldur' and 'thalach'\n",
    "df_cleaned.loc[:, 'thaldur'] = df_cleaned['thaldur'].fillna(df_cleaned['thaldur'].median())\n",
    "df_cleaned.loc[:, 'thalach'] = df_cleaned['thalach'].fillna(df_cleaned['thalach'].median())\n",
    "\n",
    "# Step 5: Replace missing values and values greater than 1 for 'fbs', 'prop', 'nitr', 'pro', 'diuretic'\n",
    "for col in ['fbs', 'prop', 'nitr', 'pro', 'diuretic']:\n",
    "    df_cleaned.loc[:, col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])\n",
    "    df_cleaned.loc[:, col] = df_cleaned[col].apply(lambda x: 1 if x > 1 else x)\n",
    "\n",
    "# Step 6: Impute missing values for 'exang' and 'slope'\n",
    "df_cleaned.loc[:, 'exang'] = df_cleaned['exang'].fillna(df_cleaned['exang'].mode()[0])\n",
    "df_cleaned.loc[:, 'slope'] = df_cleaned['slope'].fillna(df_cleaned['slope'].mode()[0])\n",
    "\n",
    "\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e1539-f5c9-436e-8946-1b8552bee405",
   "metadata": {},
   "source": [
    "Cleaning Step 3\n",
    "\n",
    "For Source 1, I used the \"Proportion of people 15 years and over who were current daily smokers by age and sex, 2022\" dataset because it allows me to impute smoking rates with a higher level of precision by considering both age and sex, which will likely yield more accurate imputation for missing values in the \"smoke\" column given that the heart disease dataset also has both age and sex information.\n",
    "\n",
    "For source 2, I used the tobacco product use - cigarettes dataset and assumed that the smoking rates by different age groups for both men and women are the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99986d-2c9f-4b7c-944f-5a9cc0db20dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7b6ef90-e7d7-45ad-9aee-1b59ca266b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex  painloc  painexer   cp  trestbps  smoke  fbs  prop  nitr  ...  \\\n",
      "0  63.0  1.0      1.0       1.0  1.0     145.0    NaN  1.0   0.0   0.0  ...   \n",
      "1  67.0  1.0      1.0       1.0  4.0     160.0    NaN  0.0   1.0   0.0  ...   \n",
      "2  67.0  1.0      1.0       1.0  4.0     120.0    NaN  0.0   1.0   0.0  ...   \n",
      "3  37.0  1.0      1.0       1.0  3.0     130.0    NaN  0.0   1.0   0.0  ...   \n",
      "4  41.0  0.0      1.0       1.0  2.0     130.0    NaN  0.0   0.0   0.0  ...   \n",
      "\n",
      "   diuretic  thaldur  thalach  exang  oldpeak  slope  smoke_imputed_source1  \\\n",
      "0       0.0     10.5    150.0    0.0      2.3    3.0                   17.4   \n",
      "1       0.0      9.5    108.0    1.0      1.5    2.0                    9.9   \n",
      "2       0.0      8.5    129.0    1.0      2.6    2.0                    9.9   \n",
      "3       0.0     13.0    187.0    0.0      3.5    3.0                   13.5   \n",
      "4       0.0      7.0    172.0    0.0      1.4    1.0                   13.5   \n",
      "\n",
      "   smoke_imputed_source2  smoke_imputed_source1_normalized  \\\n",
      "0                 19.932                          1.000000   \n",
      "1                 11.484                          0.568966   \n",
      "2                 11.484                          0.568966   \n",
      "3                 16.500                          0.775862   \n",
      "4                 16.500                          0.775862   \n",
      "\n",
      "   smoke_imputed_source2_normalized  \n",
      "0                          1.000000  \n",
      "1                          0.576159  \n",
      "2                          0.576159  \n",
      "3                          0.827815  \n",
      "4                          0.827815  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Updated smoking rates for Source 1 and Source 2\n",
    "smoking_rates_source1 = {\n",
    "    '15–17': {'Male': 1.2, 'Female': 1.8},\n",
    "    '18–24': {'Male': 9.3, 'Female': 5.9},\n",
    "    '25–34': {'Male': 13.4, 'Female': 8.8},\n",
    "    '35–44': {'Male': 13.5, 'Female': 8.5},\n",
    "    '45–54': {'Male': 15.3, 'Female': 11.6},\n",
    "    '55–64': {'Male': 17.4, 'Female': 12.0},\n",
    "    '65–74': {'Male': 9.9, 'Female': 7.9},\n",
    "    '75+': {'Male': 3.8, 'Female': 1.9}\n",
    "}\n",
    "\n",
    "smoking_rates_source2 = {\n",
    "    '18–24': {'Male': 4.8, 'Female': 4.8},\n",
    "    '25–44': {'Male': 12.5, 'Female': 12.5},\n",
    "    '45–64': {'Male': 15.1, 'Female': 15.1},\n",
    "    '65+': {'Male': 8.7, 'Female': 8.7}\n",
    "}\n",
    "\n",
    "source2_smoking_rate_among_men = 13.2\n",
    "source2_smoking_rate_among_women = 10.0\n",
    "\n",
    "# Convert the 'age' column to numeric, coercing any non-numeric values to NaN\n",
    "df_cleaned['age'] = pd.to_numeric(df_cleaned['age'], errors='coerce')\n",
    "\n",
    "# For Source 1, simply replace missing values with the corresponding smoking rates for age groups\n",
    "def impute_smoking_rate_source1(row):\n",
    "    age_group = None\n",
    "    if 15 <= row['age'] <= 17:\n",
    "        age_group = '15–17'\n",
    "    elif 18.0 <= row['age'] <= 24.0:\n",
    "        age_group = '18–24'\n",
    "    elif 25.0 <= row['age'] <= 34.0:\n",
    "        age_group = '25–34'\n",
    "    elif 35.0 <= row['age'] <= 44.0:\n",
    "        age_group = '35–44'\n",
    "    elif 45.0 <= row['age'] <= 54.0:\n",
    "        age_group = '45–54'\n",
    "    elif 55.0 <= row['age'] <= 64.0:\n",
    "        age_group = '55–64'\n",
    "    elif 65 <= row['age'] <= 74:\n",
    "        age_group = '65–74'\n",
    "    elif row['age'] >= 75:\n",
    "        age_group = '75+'\n",
    "\n",
    "    if age_group is None:\n",
    "        return 0\n",
    "\n",
    "    if row['sex'] == '0':  # Female\n",
    "        return smoking_rates_source1[age_group]['Female']\n",
    "    else:  # Male\n",
    "        return smoking_rates_source1[age_group]['Male']\n",
    "\n",
    "# For Source 2, apply the formula for males\n",
    "def impute_smoking_rate_source2(row):\n",
    "    age_group = None\n",
    "    if 18 <= row['age'] <= 24:\n",
    "        age_group = '18–24'\n",
    "    elif 25 <= row['age'] <= 44:\n",
    "        age_group = '25–44'\n",
    "    elif 45 <= row['age'] <= 64:\n",
    "        age_group = '45–64'\n",
    "    elif row['age'] >= 65:\n",
    "        age_group = '65+'\n",
    "\n",
    "    if age_group is None:\n",
    "        return 0\n",
    "\n",
    "    if row['sex'] == '0':  # Female\n",
    "        return smoking_rates_source2[age_group]['Female']\n",
    "    else:  # Male\n",
    "        male_rate = smoking_rates_source2[age_group]['Male']\n",
    "        female_rate = smoking_rates_source2[age_group]['Female']\n",
    "        return male_rate * (source2_smoking_rate_among_men / source2_smoking_rate_among_women)\n",
    "\n",
    "# Apply the imputation for Source 1 and Source 2\n",
    "df_cleaned['smoke_imputed_source1'] = df_cleaned.apply(impute_smoking_rate_source1, axis=1)\n",
    "df_cleaned['smoke_imputed_source2'] = df_cleaned.apply(impute_smoking_rate_source2, axis=1)\n",
    "\n",
    "# Normalize the imputed columns\n",
    "df_cleaned['smoke_imputed_source1_normalized'] = (df_cleaned['smoke_imputed_source1'] - df_cleaned['smoke_imputed_source1'].min()) / (df_cleaned['smoke_imputed_source1'].max() - df_cleaned['smoke_imputed_source1'].min())\n",
    "df_cleaned['smoke_imputed_source2_normalized'] = (df_cleaned['smoke_imputed_source2'] - df_cleaned['smoke_imputed_source2'].min()) / (df_cleaned['smoke_imputed_source2'].max() - df_cleaned['smoke_imputed_source2'].min())\n",
    "\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9c04403-b10f-45eb-8654-2d19d193abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "142e9cf7-b327-4599-8685-03efa113d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "# Features (X) are the columns in df_cleaned\n",
    "X = df_cleaned  # Features\n",
    "\n",
    "# Target (y) is the 'target' column from the original df\n",
    "y = df['target']  # Target labels\n",
    "\n",
    "# Drop rows where the target (y) is NaN\n",
    "X = X[y.notna()]  # Keep only the rows where y is not NaN\n",
    "y = y.dropna()  # Remove NaN values from y\n",
    "\n",
    "# Perform stratified split (90-10 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbaba39a-18ba-4737-86b0-245fe51a3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91b4bda1-34fd-4711-83a6-af5105fda8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using the median\n",
    "imputer = SimpleImputer(strategy='median') \n",
    "X_imputed = imputer.fit_transform(X)  # Apply imputation to X\n",
    "\n",
    "# train-test split and model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.1, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3849925-16b9-41c2-8668-ce58a0d29920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n",
      "Training Random Forest\n",
      "Training SVM\n",
      "Training KNN\n",
      "Logistic Regression cross-validation results:\n",
      "Accuracy: Mean=0.8084, Std=0.0319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        40\n",
      "         1.0       0.72      0.84      0.78        50\n",
      "\n",
      "    accuracy                           0.73        90\n",
      "   macro avg       0.74      0.72      0.72        90\n",
      "weighted avg       0.74      0.73      0.73        90\n",
      "\n",
      "ROC AUC: 0.7947499999999998\n",
      "\n",
      "Random Forest cross-validation results:\n",
      "Accuracy: Mean=0.8060, Std=0.0264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        40\n",
      "         1.0       0.72      0.84      0.78        50\n",
      "\n",
      "    accuracy                           0.73        90\n",
      "   macro avg       0.74      0.72      0.72        90\n",
      "weighted avg       0.74      0.73      0.73        90\n",
      "\n",
      "ROC AUC: 0.7947499999999998\n",
      "\n",
      "SVM cross-validation results:\n",
      "Accuracy: Mean=0.7960, Std=0.0343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        40\n",
      "         1.0       0.72      0.84      0.78        50\n",
      "\n",
      "    accuracy                           0.73        90\n",
      "   macro avg       0.74      0.72      0.72        90\n",
      "weighted avg       0.74      0.73      0.73        90\n",
      "\n",
      "ROC AUC: 0.7947499999999998\n",
      "\n",
      "KNN cross-validation results:\n",
      "Accuracy: Mean=0.7750, Std=0.0275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        40\n",
      "         1.0       0.72      0.84      0.78        50\n",
      "\n",
      "    accuracy                           0.73        90\n",
      "   macro avg       0.74      0.72      0.72        90\n",
      "weighted avg       0.74      0.73      0.73        90\n",
      "\n",
      "ROC AUC: 0.7947499999999998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 4\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Apply scaling to training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply the same scaling to test data\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Increase max_iter to 1000\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None]\n",
    "}\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# 5-fold cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# For each model, perform 5-fold cross-validation and hyperparameter tuning\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}\")\n",
    "    \n",
    "    if model_name == 'Random Forest':\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid_rf, cv=cv, scoring='accuracy')\n",
    "        grid_search.fit(X_train_scaled, y_train)  # Use scaled data for training\n",
    "        best_model = grid_search.best_estimator_\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid_lr, cv=cv, scoring='accuracy')\n",
    "        grid_search.fit(X_train_scaled, y_train)  # Use scaled data for training\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        best_model = model  # No hyperparameter tuning for SVM and KNN for simplicity\n",
    "        \n",
    "    # 5-fold cross-validation\n",
    "    cv_results = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    results[model_name] = cv_results\n",
    "\n",
    "# Reporting performance\n",
    "for model_name, cv_results in results.items():\n",
    "    print(f\"{model_name} cross-validation results:\")\n",
    "    print(f\"Accuracy: Mean={cv_results.mean():.4f}, Std={cv_results.std():.4f}\")\n",
    "\n",
    "    # Train on the full dataset and assess performance\n",
    "    best_model.fit(X_train_scaled, y_train)  # Train using the scaled data\n",
    "    y_pred = best_model.predict(X_test_scaled)  # Make predictions on the scaled test data\n",
    "    \n",
    "    # Print classification report (Precision, Recall, F1-score)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Calculate and print ROC AUC\n",
    "    if hasattr(best_model, \"predict_proba\"):  # Check if the model supports predict_proba method\n",
    "        roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "    else:\n",
    "        roc_auc = \"Not available\"  # Some models like SVM don't support predict_proba\n",
    "    print(f\"ROC AUC: {roc_auc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3cfcb-47de-4249-a32f-8085bc9ad331",
   "metadata": {},
   "source": [
    "Key Performance Metrics:\n",
    "- Accuracy: The proportion of correctly predicted labels.\n",
    "- Precision: The proportion of positive predictions that were actually correct.\n",
    "- Recall: The proportion of actual positives that were correctly identified.\n",
    "- F1-score: The harmonic mean of precision and recall.\n",
    "- ROC AUC: The area under the Receiver Operating Characteristic curve, which provides an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa101b3-35d5-410a-b3e2-83a06caeb7d8",
   "metadata": {},
   "source": [
    "Summary of Results\n",
    "\n",
    "- Logistic Regression, Random Forest, SVM, and KNN all show similar cross-validation accuracy, around 80% for training, with slight variations.\n",
    "\n",
    "Performance Metrics (Precision, Recall, F1-Score):\n",
    "\n",
    "- Precision: All models have similar precision for both classes, around 0.72–0.75.\n",
    "- Recall: Recall for the positive class (1.0) is around 0.84, which is quite good.\n",
    "- F1-Score: The F1-score, which balances both precision and recall, is also consistent across models, around 0.73–0.78.\n",
    "- ROC AUC: All models report an ROC AUC of ~0.795, indicating a good level of model performance. ROC AUC is particularly valuable because it assesses model performance across all possible thresholds, making it a good indicator of a model’s discriminatory ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851a2fa-8c5e-4174-9e4c-4447c10002bd",
   "metadata": {},
   "source": [
    "Recommendations:\n",
    "\n",
    "- Logistic Regression and Random Forest are performing quite similarly in terms of accuracy, precision, recall, and F1-score, so you could consider focusing on one of these models depending on other factors (e.g., interpretability, speed).\n",
    "- SVM and KNN perform slightly worse in terms of accuracy, though the difference is not drastic.\n",
    "- ROC AUC of ~0.795 is relatively strong, suggesting that the models can distinguish between the two classes fairly well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b089fa4-6b01-4c0b-a617-bc08ae9d206c",
   "metadata": {},
   "source": [
    "Task 5 (Final Model Selection)\n",
    "\n",
    "Given that your main goal is to achieve good performance in detecting heart disease (where detecting positive cases is crucial), Random Forest strikes a good balance between performance and interpretability (through feature importance). It is well-suited for this kind of task, and its robustness to overfitting and ability to capture complex relationships in the data makes it a good model. It does well in terms of accuracy, precision, recall, and F1-score and the ROC AUC of around 0.795 shows that the model has a strong ability to distinguish between the classes, which is important in medical applications like heart disease prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c5eb5-f8f3-4dcb-a96b-8c220c5ca051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
